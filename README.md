# BLOG_GENERATOR_End-To-End-LLM-Project-Using-LLAMA-2_Open-Source_LLM_Model

The Llama 2 release introduces a family of pretrained and fine-tuned LLMs, ranging in scale from 7B to 70B parameters (7B, 13B, 70B). The pretrained models come with significant improvements over the Llama 1 models, including being trained on 40% more tokens, having a much longer context length (4k tokens ðŸ¤¯), and using grouped-query attention for fast inference of the 70B modelðŸ”¥!
